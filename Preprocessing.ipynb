{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/reyashkadyan/Documents/GitHub/cp-decomposition'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING DATA\n",
    "lookup_df = pd.read_excel('/Users/reyashkadyan/Desktop/Research/Stream-data/Lookuptable.xlsx')\n",
    "q1_df = pd.read_excel('/Users/reyashkadyan/Desktop/Research/Stream-data/Streams BI Data_Query 1/West Bound/simple measures from BI_WB July 2020.xlsx', skiprows = 6, header = [0,1,2])\n",
    "# q2_df = pd.read_excel('/Users/reyashkadyan/Desktop/Research/Stream-data/Streams BI Data_Query 2/Streams BI_NPI Raw Measures WB Jan 20-Jan 21.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q1 (679, 279)\n",
      "Revised hape of Q1 (679, 160)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Q1\", q1_df.shape)\n",
    "\n",
    "# Drop all completely empty columns\n",
    "q1_df.dropna(how='all', axis=1, inplace = True)\n",
    "\n",
    "print(\"Revised hape of Q1\", q1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Columns Texts 160\n",
      "Length of New Columns Texts 160\n"
     ]
    }
   ],
   "source": [
    "def clean_header(col):\n",
    "    col1, col2, col3 = col.split(\"|\")\n",
    "    \n",
    "    if 'Travel Time' in col1:\n",
    "        col1 = 'travel_time' \n",
    "    elif 'Posted Speed' in col1:\n",
    "        col1 = 'post_speed'\n",
    "    elif 'Travel Speed' in col1:\n",
    "        col1 = 'travel_speed'\n",
    "    elif col1 != '':\n",
    "        col1 = '_'.join(col1.split(' ')[1:3]).lower()\n",
    "    else:\n",
    "        col1 = ''\n",
    "        \n",
    "    col2 = datetime.strftime(datetime.strptime(col2, \"%d %b %y\"), '%d-%m-%Y') if col2 != '' else ''\n",
    "    col3 = col3.lower().replace(' ', '_') if col3.isupper() == False else ''\n",
    "    # making new column label\n",
    "    new_col = '%s%s%s' % ('%s ' % col1 if col1!= '' else '', '%s_' % col2 if col2!= '' else '','%s' % col3 if col3!= '' else '')\n",
    "    return new_col.strip('_')\n",
    "\n",
    "bound = 'W'\n",
    "\n",
    "# FORMAT HEADER\n",
    "column_text = ['%s%s%s' % ('%s|' % a if 'Unnamed' not in b else '|', '%s|' % b if 'Unnamed' not in b else '|', '%s' % c if 'Unnamed' not in c else '') for a, b, c in q1_df.columns]\n",
    "print('Length of Columns Texts', len(column_text))\n",
    "\n",
    "new_column_text = [clean_header(each) for each in column_text]\n",
    "print('Length of New Columns Texts', len(new_column_text))\n",
    "\n",
    "# setting new column names\n",
    "q1_df.columns = new_column_text\n",
    "\n",
    "# FORWARD FILL COMBINED CELLS\n",
    "# Forward filling for selected columns\n",
    "q1_df.row_labels = q1_df.row_labels.fillna(method = 'ffill')\n",
    "q1_df.npi_avg_posted_speed = q1_df.npi_avg_posted_speed.fillna(method = 'ffill')\n",
    "q1_df.npi_link_length = q1_df.npi_link_length.fillna(method = 'ffill')\n",
    "q1_df.npi_link_type = q1_df.npi_link_type.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MELTING DATA\n",
    "fixed_cols = ['row_labels', 'npi_avg_posted_speed', 'npi_link_length', 'npi_link_type', '15_minute_interval']\n",
    "\n",
    "# extracting columns for each metric\n",
    "avg_flow_cols = fixed_cols + [each for each in new_column_text if 'avg_flow' in each]\n",
    "post_speed_cols = fixed_cols + [each for each in new_column_text if 'post_speed' in each]\n",
    "travel_speed_cols = fixed_cols + [each for each in new_column_text if 'travel_speed' in each]\n",
    "travel_time_cols = fixed_cols + [each for each in new_column_text if 'travel_time' in each]\n",
    "congested_minutes_cols = fixed_cols + [each for each in new_column_text if 'congested_minutes' in each]\n",
    "\n",
    "# melting all metric related columns into rows based on fixed columns\n",
    "avg_flow_df = q1_df[avg_flow_cols].melt(fixed_cols, var_name='date', value_name='avg_flow')\n",
    "post_speed_df = q1_df[post_speed_cols].melt(fixed_cols, var_name='date', value_name='post_speed')\n",
    "travel_speed_df = q1_df[travel_speed_cols].melt(fixed_cols, var_name='date', value_name='travel_speed')\n",
    "travel_time_df = q1_df[travel_time_cols].melt(fixed_cols, var_name='date', value_name='travel_time')\n",
    "congested_minutes_df = q1_df[congested_minutes_cols].melt(fixed_cols, var_name='date', value_name='congested_minutes')\n",
    "\n",
    "# fixing dates\n",
    "avg_flow_df.date = avg_flow_df.date.apply(lambda x: x.split(' ')[1])\n",
    "post_speed_df.date = post_speed_df.date.apply(lambda x: x.split(' ')[1])\n",
    "travel_speed_df.date = travel_speed_df.date.apply(lambda x: x.split(' ')[1])\n",
    "travel_time_df.date = travel_time_df.date.apply(lambda x: x.split(' ')[1])\n",
    "congested_minutes_df.date = congested_minutes_df.date.apply(lambda x: x.split(' ')[1])\n",
    "\n",
    "# check the shape of all metrics\n",
    "# print(avg_flow_df.shape, post_speed_df.shape, travel_speed_df.shape, travel_time_df.shape, congested_minutes_df.shape)\n",
    "\n",
    "# MERGING MELTED DATA\n",
    "# pd.merge(avg_flow_df, post_speed_df, on = fixed_cols + ['date'])\n",
    "data_frames = [avg_flow_df, post_speed_df, travel_speed_df, travel_time_df, congested_minutes_df]\n",
    "\n",
    "# merging all the metric dataframes\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left, right, on = fixed_cols + ['date']), data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING WITH LOOKUP\n",
    "\n",
    "# cleaning road names to be used in join with LookUp table\n",
    "lookup_df.Road = lookup_df.Road.apply(lambda x: x.strip())\n",
    "df_merged.row_labels = df_merged.row_labels.apply(lambda x: x.strip())\n",
    "# lookup_df[lookup_df.Bound == 'W']\n",
    "\n",
    "# merging with the LookUp\n",
    "q1_df_merged = pd.merge(df_merged, lookup_df[lookup_df.Bound == bound], how = 'left', left_on = 'row_labels', right_on = \"Road\")\n",
    "\n",
    "# CALCULATED COLUMNS\n",
    "# splitting the time column into two\n",
    "q1_df_merged[['StartTime', 'EndTime']] = q1_df_merged['15_minute_interval'].str.split(' - ', 1, expand=True)\n",
    "\n",
    "# computing travel time for 10Kms\n",
    "q1_df_merged['TT_10Kms'] = ((q1_df_merged.travel_time/q1_df_merged.npi_link_length)*1000)*10\n",
    "\n",
    "# FINAL Q1 DATA\n",
    "# filtering out desired columns\n",
    "q1 = q1_df_merged[['date', 'O', 'D', 'npi_avg_posted_speed', 'npi_link_length', 'StartTime', 'EndTime', 'avg_flow', \\\n",
    "                   'travel_speed', 'travel_time', 'TT_10Kms', 'congested_minutes']]\n",
    "# Rename columns as desired\n",
    "q1.columns = ['Date', 'O', 'D', 'PostSpd', 'Length', 'StartTime', 'EndTime', 'Flow', 'Speed', 'AvgTT', 'TT_10Kms', 'CongMin']\n",
    "\n",
    "# dropping the rows with empty Origin and Destination\n",
    "q1 = q1.dropna(subset=['O', 'D'], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NPI(Metro) - 13 - Finucane Rd WB',\n",
       "       'Shore St West WB between Grant St & Finucane Rd',\n",
       "       'Finucane Rd WB between Willard Rd & Old Clevelan Old Cleve Ea Moreton Bay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_df_merged[q1_df_merged.D.isna() == True].row_labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_header(col):\n",
    "    col1, col2, col3 = col.split(\"|\")\n",
    "    \n",
    "    if 'Travel Time' in col1:\n",
    "        col1 = 'travel_time' \n",
    "    elif 'Posted Speed' in col1:\n",
    "        col1 = 'post_speed'\n",
    "    elif 'Travel Speed' in col1:\n",
    "        col1 = 'travel_speed'\n",
    "    elif col1 != '':\n",
    "        col1 = '_'.join(col1.split(' ')[1:3]).lower()\n",
    "    else:\n",
    "        col1 = ''\n",
    "        \n",
    "    col2 = datetime.strftime(datetime.strptime(col2, \"%d %b %y\"), '%d-%m-%Y') if col2 != '' else ''\n",
    "    col3 = col3.lower().replace(' ', '_') if col3.isupper() == False else ''\n",
    "    # making new column label\n",
    "    new_col = '%s%s%s' % ('%s ' % col1 if col1!= '' else '', '%s_' % col2 if col2!= '' else '','%s' % col3 if col3!= '' else '')\n",
    "    return new_col.strip('_')\n",
    "\n",
    "def prepare_Q1(file_name, bound, lookup_df):\n",
    "    \n",
    "    ## 1. READING DATA\n",
    "    q1_df = pd.read_excel(file_name, skiprows = 6, header = [0,1,2])\n",
    "    \n",
    "    ## 2. DROP EMPTY COLUMNS\n",
    "    q1_df.dropna(how='all', axis=1, inplace = True)\n",
    "    \n",
    "    ## 3. FORMAT HEADER\n",
    "    column_text = ['%s%s%s' % ('%s|' % a if 'Unnamed' not in b else '|', '%s|' % b if 'Unnamed' not in b else '|', '%s' % c if 'Unnamed' not in c else '') for a, b, c in q1_df.columns]\n",
    "    new_column_text = [clean_header(each) for each in column_text]\n",
    "\n",
    "    # setting new column names\n",
    "    q1_df.columns = new_column_text\n",
    "\n",
    "    ## 4. FORWARD FILL COMBINED CELLS\n",
    "    # Forward filling for selected columns\n",
    "    q1_df.row_labels = q1_df.row_labels.fillna(method = 'ffill')\n",
    "    q1_df.npi_avg_posted_speed = q1_df.npi_avg_posted_speed.fillna(method = 'ffill')\n",
    "    q1_df.npi_link_length = q1_df.npi_link_length.fillna(method = 'ffill')\n",
    "    q1_df.npi_link_type = q1_df.npi_link_type.fillna(method = 'ffill')\n",
    "    \n",
    "    ## 5. MELTING DATA\n",
    "    fixed_cols = ['row_labels', 'npi_avg_posted_speed', 'npi_link_length', 'npi_link_type', '15_minute_interval']\n",
    "\n",
    "    # extracting columns for each metric\n",
    "    avg_flow_cols = fixed_cols + [each for each in new_column_text if 'avg_flow' in each]\n",
    "    post_speed_cols = fixed_cols + [each for each in new_column_text if 'post_speed' in each]\n",
    "    travel_speed_cols = fixed_cols + [each for each in new_column_text if 'travel_speed' in each]\n",
    "    travel_time_cols = fixed_cols + [each for each in new_column_text if 'travel_time' in each]\n",
    "    congested_minutes_cols = fixed_cols + [each for each in new_column_text if 'congested_minutes' in each]\n",
    "\n",
    "    # melting all metric related columns into rows based on fixed columns\n",
    "    avg_flow_df = q1_df[avg_flow_cols].melt(fixed_cols, var_name='date', value_name='avg_flow')\n",
    "    post_speed_df = q1_df[post_speed_cols].melt(fixed_cols, var_name='date', value_name='post_speed')\n",
    "    travel_speed_df = q1_df[travel_speed_cols].melt(fixed_cols, var_name='date', value_name='travel_speed')\n",
    "    travel_time_df = q1_df[travel_time_cols].melt(fixed_cols, var_name='date', value_name='travel_time')\n",
    "    congested_minutes_df = q1_df[congested_minutes_cols].melt(fixed_cols, var_name='date', value_name='congested_minutes')\n",
    "\n",
    "    # fixing dates\n",
    "    avg_flow_df.date = avg_flow_df.date.apply(lambda x: x.split(' ')[1])\n",
    "    post_speed_df.date = post_speed_df.date.apply(lambda x: x.split(' ')[1])\n",
    "    travel_speed_df.date = travel_speed_df.date.apply(lambda x: x.split(' ')[1])\n",
    "    travel_time_df.date = travel_time_df.date.apply(lambda x: x.split(' ')[1])\n",
    "    congested_minutes_df.date = congested_minutes_df.date.apply(lambda x: x.split(' ')[1])\n",
    "\n",
    "    # check the shape of all metrics\n",
    "    # print(avg_flow_df.shape, post_speed_df.shape, travel_speed_df.shape, travel_time_df.shape, congested_minutes_df.shape)\n",
    "\n",
    "    ## 6. MERGING MELTED DATA\n",
    "    data_frames = [avg_flow_df, post_speed_df, travel_speed_df, travel_time_df, congested_minutes_df]\n",
    "\n",
    "    # merging all the metric dataframes\n",
    "    df_merged = reduce(lambda  left,right: pd.merge(left, right, on = fixed_cols + ['date']), data_frames)\n",
    "    \n",
    "    ## 7. MERGING WITH LOOKUP\n",
    "    # cleaning road names to be used in join with LookUp table\n",
    "    lookup_df.Road = lookup_df.Road.apply(lambda x: x.strip())\n",
    "    df_merged.row_labels = df_merged.row_labels.apply(lambda x: x.strip())\n",
    "\n",
    "    # merging with the LookUp\n",
    "    q1_df_merged = pd.merge(df_merged, lookup_df[lookup_df.Bound == bound], how = 'left', left_on = 'row_labels', right_on = \"Road\")\n",
    "\n",
    "    ## 8. CALCULATED COLUMNS\n",
    "    # splitting the time column into two\n",
    "    q1_df_merged[['StartTime', 'EndTime']] = q1_df_merged['15_minute_interval'].str.split(' - ', 1, expand=True)\n",
    "\n",
    "    # computing travel time for 10Kms\n",
    "    q1_df_merged['TT_10Kms'] = ((q1_df_merged.travel_time/q1_df_merged.npi_link_length)*1000)*10\n",
    "\n",
    "    ## 9. FINAL Q1 DATA\n",
    "    # filtering out desired columns\n",
    "    q1 = q1_df_merged[['date', 'O', 'D', 'npi_avg_posted_speed', 'npi_link_length', 'StartTime', 'EndTime', 'avg_flow', \\\n",
    "                       'travel_speed', 'travel_time', 'TT_10Kms', 'congested_minutes']]\n",
    "    # Rename columns as desired\n",
    "    q1.columns = ['Date', 'O', 'D', 'PostSpd', 'Length', 'StartTime', 'EndTime', 'Flow', 'Speed', 'AvgTT', 'TT_10Kms', 'CongMin']\n",
    "\n",
    "    # dropping the rows with empty Origin and Destination\n",
    "    q1 = q1.dropna(subset=['O', 'D'], axis = 0)\n",
    "    \n",
    "    ## 10. RETURN PRE-PROCESSED Q1 DATA\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTED: Working for both West and East Bound\n",
    "\n",
    "# load lookup table\n",
    "Lookup = pd.read_excel('/Users/reyashkadyan/Desktop/Research/Stream-data/Lookuptable.xlsx')\n",
    "\n",
    "# load the name of the file to be processed here\n",
    "File_path = '/Users/reyashkadyan/Desktop/Research/Stream-data/Streams BI Data_Query 1/East Bound/simple measures from BI_EB Feb 2020.xlsx'\n",
    "\n",
    "# change the traffic direction based on the file used above\n",
    "Bound = 'E'\n",
    "\n",
    "# Pre-process\n",
    "Q1 = prepare_Q1(file_name = File_path, bound = Bound, lookup_df = Lookup)\n",
    "\n",
    "# writing the file to excel\n",
    "Q1.to_excel('type_your_file_name_here.xlsx', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
